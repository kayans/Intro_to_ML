# -*- coding: utf-8 -*-
"""IntroML_HW4_KayanShih_part1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19d3Wq-6yQsf8PTIZKcUFxzXOzAkrKRuU
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

count_WORD=1
count_CHAR=1
column_names=[]
for i in range(58):
    if i < 48:
        column_names.append(f"word_freq_WORD_{count_WORD}")
        count_WORD+=1
    elif 48 <= i < 54:
        column_names.append(f"char_freq_CHAR_{count_CHAR}")
        count_CHAR+=1
    elif i == 54:
        column_names.append("capital_run_length_average")
    elif i == 55:
        column_names.append("capital_run_length_longest")
    elif i == 56:
        column_names.append("capital_run_length_total")
    elif i == 57:
        column_names.append("class")

#read file: spam (class 1) or not spam (class 0)
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data"
data = pd.read_csv(url, sep=',', names=[i for i in column_names])
X = data.drop('class',axis=1)
y = data['class']

best_test_accuracy_gini = 0
random_state_gini = 0
best_test_accuracy_ig = 0
random_state_ig = 0

for i in range(20):
    #setting the “stratify = y” ensures that both the train and test sets have the proportion of examples in each class that is present in the provided “y” array.
    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i, shuffle=True, stratify=y)
    #normalize based on the training set statistics (mean/std) on both the training and testing data
    scaler = StandardScaler().fit(x_train)
    x_train_sc = pd.DataFrame(scaler.transform(x_train))
    x_test_sc = pd.DataFrame(scaler.transform(x_test))
    
    #Gini Impurity
    clf_gini = DecisionTreeClassifier(criterion = "gini", random_state =i)
    clf_gini = clf_gini.fit(x_train_sc,y_train)
    y_pred_gini = clf_gini.predict(x_test_sc)
    acc_gini = accuracy_score(y_test, y_pred_gini)
    if acc_gini > best_test_accuracy_gini:
        best_test_accuracy_gini = acc_gini
        random_state_gini = i

    #Shannon Information Gain
    clf_ig = DecisionTreeClassifier(criterion = "entropy", random_state =i)
    clf_ig = clf_ig.fit(x_train_sc,y_train)
    y_pred_ig = clf_ig.predict(x_test_sc)
    acc_ig = accuracy_score(y_test, y_pred_ig)
    if acc_ig > best_test_accuracy_ig:
      best_test_accuracy_ig = acc_ig
      random_state_ig = i

print(f'The best Test Accuracy using Gini Impurity is {best_test_accuracy_gini:.4f} at random state {random_state_gini}.')
print(f'The best Test Accuracy using Shannon I.G. is {best_test_accuracy_ig:.4f} at random state {random_state_ig}.')