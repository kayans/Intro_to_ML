# -*- coding: utf-8 -*-
"""IntroML_HW4_KayanShih_part2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ygL25p1IPmWCimdmeNDMXyUiJ5RYUBle
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

count_WORD=1
count_CHAR=1
column_names=[]
for i in range(58):
    if i < 48:
        column_names.append(f"word_freq_WORD_{count_WORD}")
        count_WORD+=1
    elif 48 <= i < 54:
        column_names.append(f"char_freq_CHAR_{count_CHAR}")
        count_CHAR+=1
    elif i == 54:
        column_names.append("capital_run_length_average")
    elif i == 55:
        column_names.append("capital_run_length_longest")
    elif i == 56:
        column_names.append("capital_run_length_total")
    elif i == 57:
        column_names.append("class")

#read file: spam (class 1) or not spam (class 0)
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data"
data = pd.read_csv(url, sep=',', names=[i for i in column_names])
X = data.drop('class',axis=1)
y = data['class']

import tqdm

n_estimators = [1,3,5,10,15,20,40,70]

rs_gini = []
rs_ig = []
best_test_acc_gini = []
best_test_acc_ig = []

for n in tqdm.tqdm(n_estimators):
  best_test_accuracy_gini = 0
  best_test_accuracy_ig = 0
  random_state_gini = 0
  random_state_ig = 0
  for i in range(20):
    #setting the “stratify = y” ensures that both the train and test sets have the proportion of examples in each class that is present in the provided “y” array.
    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i, shuffle=True, stratify=y)
    #normalize based on the training set statistics (mean/std) on both the training and testing data
    scaler = StandardScaler().fit(x_train)
    x_train_sc = pd.DataFrame(scaler.transform(x_train))
    x_test_sc = pd.DataFrame(scaler.transform(x_test))
    
    #Gini Impurity
    clf_gini = RandomForestClassifier(n_estimators = n, criterion='gini', random_state=i)
    clf_gini = clf_gini.fit(x_train_sc,y_train)
    y_pred_gini = clf_gini.predict(x_test_sc)
    acc_gini = accuracy_score(y_test, y_pred_gini)
    if acc_gini > best_test_accuracy_gini:
      best_test_accuracy_gini = acc_gini
      random_state_gini = i
        
    #Shannon Information Gain
    clf_ig = RandomForestClassifier(n_estimators = n, criterion='entropy', random_state=i)
    clf_ig = clf_ig.fit(x_train_sc,y_train)
    y_pred_ig = clf_ig.predict(x_test_sc)
    acc_ig = accuracy_score(y_test, y_pred_ig)
    if acc_ig > best_test_accuracy_ig:
      best_test_accuracy_ig = acc_ig
      random_state_ig = i
  best_test_acc_gini.append(best_test_accuracy_gini)
  rs_gini.append(random_state_gini)
  best_test_acc_ig.append(best_test_accuracy_ig)
  rs_ig.append(random_state_ig)

for i in range(len(n_estimators)):
  print(f'When the number of estimators is {n_estimators[i]}')
  print(f'The best Test Accuracy using Gini Impurity is {best_test_acc_gini[i]:.4f} at random state {rs_gini[i]}.')
  print(f'The best Test Accuracy using Shannon I.G. is {best_test_acc_ig[i]:.4f} at random state {rs_ig[i]}.')