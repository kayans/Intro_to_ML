{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Neural Networks (Extra credit - 25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question : Multi-layer perceptron training using backpropagation\n",
    "\n",
    "In this part of the homework you will implement a multi-layer perceptron model and train it using the backpropagation algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the packages. Please do not import any other packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and splitting the data\n",
    "We will load and work with the half moons data set from sklearn and train a multi-layer perceptron to distinguish between the two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples in the data set\n",
    "N_SAMPLES = 1000\n",
    "# ratio between training and test sets\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "# Double moon dataset\n",
    "X, y = make_moons(n_samples = N_SAMPLES, noise=0.2, random_state=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)\n",
    "\n",
    "def make_plot(X, y, plot_name, file_name=None, XX=None, YY=None, preds=None, dark=False):\n",
    "    plt.figure(figsize=(16,12))\n",
    "    axes = plt.gca()\n",
    "    axes.set(xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
    "    plt.title(plot_name, fontsize=30)\n",
    "    plt.subplots_adjust(left=0.20)\n",
    "    plt.subplots_adjust(right=0.80)\n",
    "    if(XX is not None and YY is not None and preds is not None):\n",
    "        plt.contourf(XX, YY, preds.reshape(XX.shape), 25, alpha = 1, cmap=cm.Spectral)\n",
    "        plt.contour(XX, YY, preds.reshape(XX.shape), levels=[.5], cmap=\"Greys\", vmin=0, vmax=.6)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=40, cmap=plt.cm.Spectral, edgecolors='black')\n",
    "    if(file_name):\n",
    "        plt.savefig(file_name)\n",
    "        plt.close()\n",
    "        \n",
    "make_plot(X, y, \"Double Moon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2.a: Create and initialize the multi-layer perceptron (5 Points)\n",
    "In this section you will implement and initialize the weight layers of the multi-layer preceptron which has the following architecture: \n",
    "\n",
    "FC_2X25 -> ReLU_layer -> FC_25X50 -> ReLU_layer -> FC_50X25 -> ReLU_layer -> FC_25X1 -> Sigmoid_layer\n",
    "\n",
    "Where FC_InpXOut refers to the fully connected layer with `Inp` input units and `Out` output units. ReLU_layer and the Sigmoid_layer are the relu and sigmoid activation functions respectively. \n",
    "\n",
    "In the `init_layers` function, initialize all trainable parameters of the MLP model.\n",
    "\n",
    "`nn_architecture` is a list of dictionaries with layer specification.\n",
    "\n",
    "`seed` defines the random seed for all initial parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the architecture of the layers specified above\n",
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 50, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "\n",
    "\n",
    "# define the initialization function\n",
    "def init_layers(nn_architecture, seed = 42):\n",
    "    # random seed initiation\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # number of layers in our neural network\n",
    "    number_of_layers = ? \n",
    "    \n",
    "    # parameters storage initiation\n",
    "    params_values = {}\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        \n",
    "        # extracting the number of units in layers\n",
    "        layer_input_size = ? \n",
    "        layer_output_size = ? \n",
    "        \n",
    "        # initiating the values of the W matrix\n",
    "        # and vector b for subsequent layers\n",
    "        params_values['W' + str(layer_idx)] = ? \n",
    "        params_values['b' + str(layer_idx)] = ? \n",
    "        \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2.b: Implement the forward and backward functions for activation functions (5 Points)\n",
    "1. Sigmoid function\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "2. ReLU function\n",
    "$$\n",
    "relu(x) = \\max\\{0, x\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function: sigmoid(X) = 1/(1 + exp(-X))\n",
    "def sigmoid(Z):\n",
    "    sig = ?\n",
    "    return sig\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    dZ = ?\n",
    "    return dZ\n",
    "\n",
    "# ReLU function: relu(Z) = max(0, Z)\n",
    "def relu(Z):\n",
    "    rel = ?\n",
    "    return rel\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = ?\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2.c: Implement the forward pass over MLP (15 Points)\n",
    "We now implement the forward pass over the entire multi-layer perceptron to compute the activations for all the units of MLP. It consists of two functions: \n",
    "1. `single_layer_forward_propagation`: forward pass over a single layer, which is composed of an FC layer followed by an activation function (either ReLU or Sigmoid)\n",
    "2. `full_forward_propagation`: forward pass over the entire MLP network which consists of looping over the layers and calling the `single_layer_forward_propagation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    \"\"\"\n",
    "    Perform forward propagation over a single layer composed of an FC layer followed by an activation layer. \n",
    "    \n",
    "    Input:\n",
    "        A_prev: np.ndarray(inpdim, nbatch_size), input activations to the current layer\n",
    "        W_curr: np.ndarray(outdim, inpdim), weights of the FC component of the current layer \n",
    "        b_curr: np.ndarray(outdim, 1), biases of the FC component of the current layer \n",
    "        activations: name of the activation layer (either \"relu\" or \"sigmoid\")\n",
    "    \n",
    "    Output:\n",
    "        A_curr: final output of the activation function of the current layer\n",
    "        Z_curr: intermediate input to the activation function\n",
    "    \"\"\"\n",
    "    # calculation of the input value for the activation function\n",
    "    Z_curr = ? \n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation == \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation == \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "    \n",
    "    # calculate the current activations \n",
    "    A_curr = activation_func(Z_curr)\n",
    "    \n",
    "    # return of calculated activation A and the intermediate Z matrix\n",
    "    return A_curr, Z_curr\n",
    "\n",
    "\n",
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    \"\"\"\n",
    "    Perform forward propagation over full MLP network composed of a single of single layers stacked on top of each other\n",
    "    \n",
    "    Input:\n",
    "        X: np.ndarray(inpdim, nbatch_size), input features to the MLP activations to the current layer\n",
    "        param_values: an array of parameters (weights and biases) returned by the init_layers function\n",
    "        nn_architecture: dictionary of architecture layers\n",
    "    \n",
    "    Output:\n",
    "        A_curr: final prediction of the network\n",
    "        memory: dictionary of intermediate input to the activation function\n",
    "    \"\"\"\n",
    "    # creating a temporary memory to store the information needed for a backward step\n",
    "    memory = {}\n",
    "    # X vector is the activation for layer 0â€Š\n",
    "    A_curr = X\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        # transfer the activation from the previous iteration\n",
    "        A_prev = ?\n",
    "        \n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = ?\n",
    "        \n",
    "        # extraction of W for the current layer\n",
    "        W_curr = ?\n",
    "        # extraction of b for the current layer\n",
    "        b_curr = ?\n",
    "        # calculation of activation for the current layer\n",
    "        A_curr, Z_curr = ?\n",
    "        \n",
    "        # saving calculated values in the memory\n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    # return of prediction vector and a dictionary containing intermediate values\n",
    "    return A_curr, memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for computing the cost function (nothing to do here)\n",
    "The cross entropy loss\n",
    "$$ L = -\\frac{1}{m} \\left(Y \\log{\\hat{Y}}^T + (1-Y)\\log{(1 - \\hat{Y})}^T \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute the cross entropy cost \n",
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost)\n",
    "\n",
    "# an auxiliary function that converts probability into class\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "\n",
    "# function to get the accuracy of the predictions\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2.d: Implement the backward pass over the MLP (15 Points)\n",
    "We now implement the backward pass over the entire multi-layer perceptron to compute the gradients with respect to the activations and the weights in the MLP. It consists of two functions: \n",
    "1. `single_layer_backward_propagation`: backward pass over a single layer, which is composed of an FC layer followed by an activation function (either ReLU or Sigmoid)\n",
    "2. `full_backward_propagation`: backward pass over the entire MLP network which consists of looping over the layers in the reverse order (starting from top) and calling the `single_layer_backward_propagation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    \"\"\"\n",
    "    Perform forward propagation over a single layer composed of an FC layer followed by an activation layer. \n",
    "    \n",
    "    Input:\n",
    "        dA_curr: gradients from the output activations of the current layer \n",
    "        W_curr: weights of the FC component of the current layer \n",
    "        b_curr: biases of the FC component of the current layer \n",
    "        Z_curr: inputs to the activation function of the current layer\n",
    "        A_prev: inputs to the current layer \n",
    "        activations: name of the activation layer (either \"relu\" or \"sigmoid\")\n",
    "    \n",
    "    Output:\n",
    "        dA_prev: gradients with respect to the inputs of the current layer\n",
    "        dW_curr: gradients with respect to the weights of the FC component of the current layer\n",
    "        db_curr: gradients with respect to the biases of the FC component of the current layer\n",
    "    \"\"\"\n",
    "    # number of examples\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation == \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation == \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "    \n",
    "    # calculation of the activation function derivative\n",
    "    dZ_curr = ?\n",
    "    \n",
    "    # derivative of the matrix W\n",
    "    dW_curr = ? \n",
    "    # derivative of the vector b\n",
    "    db_curr = ?\n",
    "    # derivative of the matrix A_prev\n",
    "    dA_prev = ?\n",
    "    \n",
    "    return dA_prev, dW_curr, db_curr\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    \"\"\"\n",
    "    Perform forward propagation over a single layer composed of an FC layer followed by an activation layer. \n",
    "    \n",
    "    Input:\n",
    "        Y_hat: the output predictions of the MLP\n",
    "        Y: the ground truth value of output\n",
    "        memory: dictionary of activations for units of all layers (computed during the full_forward_propagation)\n",
    "        params_values: dictionary of parameters (weights and biases) of all layers\n",
    "        nn_architecture: dictionary of network layers\n",
    "    \n",
    "    Output:\n",
    "        grad_values: dictionary of gradients of parameters (weights and biases) of all layers\n",
    "    \"\"\"\n",
    "    grads_values = {}\n",
    "    \n",
    "    # number of examples\n",
    "    m = Y.shape[1]\n",
    "    # a hack ensuring the same shape of the prediction vector and labels vector\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    # initiation of gradient descent algorithm\n",
    "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        # we number network layers from 1\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = ?\n",
    "        \n",
    "        dA_curr = ? \n",
    "        \n",
    "        #get the activations from memory\n",
    "        A_prev = ?\n",
    "        Z_curr = ? \n",
    "        \n",
    "        # get the values of weights and biases from current layer\n",
    "        W_curr = ? \n",
    "        b_curr = ? \n",
    "        \n",
    "        # get the gradients with respect to the inputs, weights, and biases\n",
    "        dA_prev, dW_curr, db_curr = ? \n",
    "        \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    return grads_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2.e: Implement the gradient update function (10 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "    \"\"\"\n",
    "    Perform the parameter update using the gradient descent algorithm\n",
    "    \n",
    "    Input:\n",
    "        params_values: dictionary of parameters (weights and biases for all the layers)\n",
    "        grads_values: dictionary of corresponding gradients of parameters for all layers\n",
    "        nn_architecture: the dictionry of the architecture layers \n",
    "        learning_rate: the scalar learning rate\n",
    "    \n",
    "    Output:\n",
    "        params_values: dictinoary of updated parameters (weights and biases for all the layers)\n",
    "    \"\"\"\n",
    "    # iterate over network layers and update the weights and biases\n",
    "    # TODO: Implement the update rule\n",
    "    \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top level function for training and plotting the results (nothing to do here)\n",
    "The following cell implements two top level functions for your convenience: \n",
    "\n",
    "1. `train`: top level function called to train the MLP using the dataset \n",
    "2. `plot_metric`: plot the metric as a function of training iterations \n",
    "\n",
    "Please go through the structure of each of these function carefully. In particular pay special attention to the calls to the `full_forward_propagation` and `full_backward_propagation` function and also how and what metrics are being stored for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop to train the MLP algorithm. Please carefully read the structure of this function\n",
    "def train(X, Y, nn_architecture, epochs, learning_rate, batch_size=128, verbose=False, callback=None, spam_ids=None):\n",
    "    # initiation of neural net parameters\n",
    "    params_values = init_layers(nn_architecture, 2)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    num_samples = X.shape[1]\n",
    "    num_minibatches = int(X.shape[1] / batch_size)\n",
    "    \n",
    "    # performing calculations for subsequent iterations\n",
    "    for i in range(epochs):\n",
    "        for j in range(num_minibatches):\n",
    "            # step forward\n",
    "            inds = np.random.choice(X.shape[1], batch_size // 2)\n",
    "            if spam_ids is not None:\n",
    "                # oversampling minor class\n",
    "                inds_spam = np.random.choice(spam_ids.shape[0], batch_size // 2)\n",
    "                inds_spam = spam_ids[inds_spam]\n",
    "                inds = np.concatenate((inds, inds_spam))\n",
    "            Y_hat, cashe = full_forward_propagation(X[:,inds], params_values, nn_architecture)\n",
    "\n",
    "            # step backward - calculating gradient\n",
    "            grads_values = full_backward_propagation(Y_hat, Y[:, inds], cashe, params_values, nn_architecture)\n",
    "            # updating model state\n",
    "            params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "\n",
    "        if(i % 10 == 0):\n",
    "            # calculating metrics and saving them in history\n",
    "            cost = get_cost_value(Y_hat, Y[:,inds])\n",
    "            accuracy = get_accuracy_value(Y_hat, Y[:, inds])\n",
    "            cost_history.append(cost)\n",
    "            accuracy_history.append(accuracy)\n",
    "\n",
    "            if(verbose):\n",
    "                print(\"Iteration: {:05} - cost: {:.5f} - accuracy: {:.5f}\".format(i, cost, accuracy))\n",
    "            if(callback is not None):\n",
    "                callback(i, params_values)\n",
    "            \n",
    "    return params_values, cost_history, accuracy_history\n",
    "\n",
    "\n",
    "# a simple function to plot the accuracy of the model as a function of the training iterations\n",
    "def plot_metric(metric, name):\n",
    "    x_axis = np.arange(len(metric))\n",
    "    plt.figure(figsize=(16,12))\n",
    "    axes = plt.gca()\n",
    "    axes.set(xlabel=\"$Step$\", ylabel=name)\n",
    "    plt.title(\"Learning curves\", fontsize=30)\n",
    "    plt.plot(x_axis, np.array(ah))\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top level function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_values, ch, ah = train(np.transpose(X_train), np.transpose(y_train.reshape((y_train.shape[0], 1))), NN_ARCHITECTURE, 1000, 0.01, batch_size=64, verbose=True)\n",
    "\n",
    "plot_metric(ah, 'Accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mlug')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "331px",
    "left": "710px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "fc6f77fc95e5420108aa348103e4498d1d0de016cf7e1fd7da540445454c305d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
